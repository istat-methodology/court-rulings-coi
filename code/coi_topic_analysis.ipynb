{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT5gFEfMV1qC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install nltk datasets bertopic sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXE-HZQEWPKm"
      },
      "source": [
        "# BERTopic Analysis - Court Rulings (Conflict of Interests)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc4xB_iFWIL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe2f9cb-0389-4231-8a14-d1f334c9f60b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/hdbscan/robust_single_linkage_.py:175: SyntaxWarning: invalid escape sequence '\\{'\n",
            "  $max \\{ core_k(a), core_k(b), 1/\\alpha d(a,b) \\}$.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize.punkt import PunktTrainer, PunktSentenceTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('stopwords')\n",
        "italian_stopwords = stopwords.words('italian')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB63NAwhWYAw"
      },
      "source": [
        "## Data\n",
        "Import the dataset from Hugging Face ðŸ¤—."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0GabPTbWK4o"
      },
      "outputs": [],
      "source": [
        "data = load_dataset(\"istat-ai/court-rulings-coi\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tAv1RecWmfH"
      },
      "source": [
        "## Sentence Tokenization\n",
        "Now, we need to split the full rulings into individual phrases. To do this, we train a custom tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlfK8qW7dbg0"
      },
      "outputs": [],
      "source": [
        "long_text = \"\"\n",
        "\n",
        "for text in data[\"Text\"]:\n",
        "    long_text += text\n",
        "\n",
        "trainer = PunktTrainer()\n",
        "trainer.INCLUDE_ALL_COLLOCS = True\n",
        "trainer.train(long_text)\n",
        "\n",
        "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpARovr0dgIp"
      },
      "source": [
        "Now we can apply it to the texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "picaeVxoWs_l"
      },
      "outputs": [],
      "source": [
        "doc_ids = []\n",
        "all_sentences = []\n",
        "\n",
        "for idx, text in enumerate(data[\"Clean_Text\"]):\n",
        "    sentences = tokenizer.tokenize(text)\n",
        "    sentences = [s for s in sentences if len(s) > 10]\n",
        "    all_sentences.extend(sentences)\n",
        "    doc_ids.extend([idx] * len(sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9LO_IHdynL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch5BD3HJdzRY"
      },
      "source": [
        "## BERTopic Modeling\n",
        "First, we compute the embeddings using a multilingual sentence trasformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUJbYDOQRDXi"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
        "embeddings = model.encode(all_sentences, show_progress_bar=True, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa3qaB3T6zTd"
      },
      "source": [
        "Then, we define the UMAP, HDBSCAN, and Vectorizer models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xNAtcJ8U90D"
      },
      "outputs": [],
      "source": [
        "umap_model = UMAP(n_neighbors=35, min_dist=0.01, n_components=5, metric='cosine')\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=300, min_samples=1, cluster_selection_epsilon=0.01)\n",
        "vectorizer_model = CountVectorizer(stop_words=italian_stopwords, ngram_range=(1, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGX48xZY63DU"
      },
      "source": [
        "Finally, we can fit our BERTopic model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdVd0DdwVTV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "656f823d-8226-421e-bc80-2ea5eb0fc988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-01 10:57:37,790 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2026-02-01 10:58:28,337 - BERTopic - Dimensionality - Completed âœ“\n",
            "2026-02-01 10:58:28,340 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2026-02-01 10:58:34,930 - BERTopic - Cluster - Completed âœ“\n",
            "2026-02-01 10:58:34,953 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
            "2026-02-01 10:58:48,431 - BERTopic - Representation - Completed âœ“\n"
          ]
        }
      ],
      "source": [
        "topic_model = BERTopic(\n",
        "    embedding_model=model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(all_sentences, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "P-jT3dWe2CJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJZhhoVgIHXX"
      },
      "source": [
        "Save the topic info df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW299norZdGa"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic_info().to_csv(\"coi_topics_info.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRisBLGURkho"
      },
      "source": [
        "### Reduce Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0_F1G21RWMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c9cccc-5272-4a1e-a69f-29050abb09ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:21<00:00,  2.29it/s]\n",
            "2026-02-01 10:59:54,642 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
          ]
        }
      ],
      "source": [
        "new_topics = topic_model.reduce_outliers(all_sentences, topics, strategy=\"distributions\")\n",
        "topic_model.update_topics(all_sentences, topics=new_topics, vectorizer_model=vectorizer_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta0KSQJoR1EP"
      },
      "source": [
        "Save the updated topic info df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpAFBBsIR3PY"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic_info().to_csv(\"coi_topics_info_no_outliers.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZn_jVrVR96_"
      },
      "source": [
        "### Re-map Topics to Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKHmfrdySFLZ"
      },
      "outputs": [],
      "source": [
        "document_info_df = topic_model.get_document_info(all_sentences)\n",
        "assigned_topics = document_info_df['Topic'].to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0uszAcLSbJA"
      },
      "source": [
        "Create a document topic df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od-UOF1USe8o"
      },
      "outputs": [],
      "source": [
        "document_topic_df = pd.DataFrame({\n",
        "    'Document_ID': doc_ids,\n",
        "    'Document': all_sentences,\n",
        "    'Assigned_Topic': assigned_topics\n",
        "})\n",
        "\n",
        "grouped_df = document_topic_df.groupby('Document_ID').agg({\n",
        "    'Assigned_Topic': list,\n",
        "    'Document': 'count'\n",
        "}).rename(columns={'Assigned_Topic': 'Assigned_Topics', 'Document': 'sentence_count'}).reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LYJ5gi7TisR"
      },
      "source": [
        "Now let's save both the document_topic_df and the grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWHRKWqmTWTg"
      },
      "outputs": [],
      "source": [
        "document_topic_df.to_csv(\"coi_document_topic_df.csv\", index=False)\n",
        "grouped_df.to_csv(\"coi_grouped_df.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqncqOKVzE3i"
      },
      "source": [
        "### Counts by Ruling\n",
        "Count how many rulings the topic appears in at least one time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ixq18wyyzE3i"
      },
      "outputs": [],
      "source": [
        "counts = [Counter(i) for i in grouped_df['Assigned_Topics']]\n",
        "sorted_counts = [Counter(dict(sorted(c.items(), key=lambda item: item[1], reverse=True))) for c in counts]\n",
        "\n",
        "occurrences_in_docs = []\n",
        "\n",
        "for topic in topic_model.get_topic_info()[\"Topic\"]:\n",
        "    occurrences = 0\n",
        "    for count in counts:\n",
        "        if topic in count:\n",
        "            occurrences += 1\n",
        "    occurrences_in_docs.append(occurrences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XsVqmY_zE3i"
      },
      "source": [
        "Export the CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXADsKsjzE3i"
      },
      "outputs": [],
      "source": [
        "unique_ruling_counts = pd.DataFrame(occurrences_in_docs)\n",
        "unique_ruling_counts.to_csv(\"counts_by_ruling_unique.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTQY1MpxzE3j"
      },
      "source": [
        "### Topic-Ruling Distribution\n",
        "Create a dataframe of topic distributions per ruling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_9W9-djzE3j"
      },
      "outputs": [],
      "source": [
        "df = data.to_pandas()\n",
        "df[\"Document_ID\"] = df.index\n",
        "\n",
        "grouped_df = grouped_df.merge(df[[\"Document_ID\", \"Provision_Number\", \"Provision_Year\", \" URL\"]], on=\"Document_ID\", how=\"left\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA9l09Q3zE3j"
      },
      "source": [
        "Create the final ruling dataframe with info on the topic distribution per ruling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLrkFvbAzE3j"
      },
      "outputs": [],
      "source": [
        "final_grouped_df = grouped_df[[\"Provision_Number\", \"Provision_Year\", \" URL\", \"Assigned_Topics\"]]\n",
        "\n",
        "final_grouped_df.rename(columns={\n",
        "    \"Assigned_Topics\": \"Topics\",\n",
        "    \"Provision_Number\": \"Number\",\n",
        "    \"Provision_Year\": \"Year\",\n",
        "    \" URL\": \"URL\",\n",
        "}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iOhVIfazE3k"
      },
      "source": [
        "Export the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNEPUNHezE3k"
      },
      "outputs": [],
      "source": [
        "final_grouped_df.to_csv(\"topic_ruling_dist.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export the model to huggingface."
      ],
      "metadata": {
        "id": "rkaBWXQaCl8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PUSH_TO_HF: bool = False\n",
        "\n",
        "if PUSH_TO_HF:\n",
        "    topic_model.push_to_hf_hub(\"istat-ai/coi-topic\", private=True)"
      ],
      "metadata": {
        "id": "gZFpzeIe-r7p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}