{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qT5gFEfMV1qC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install nltk datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTopic Analysis - Court Rulings (Conflict of Interests)"
      ],
      "metadata": {
        "id": "WXE-HZQEWPKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize.punkt import PunktTrainer, PunktSentenceTokenizer\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "Hc4xB_iFWIL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "Import the dataset from Hugging Face ðŸ¤—."
      ],
      "metadata": {
        "id": "xB63NAwhWYAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(\"istat-ai/court-rulings-coi\", split=\"train\")"
      ],
      "metadata": {
        "id": "L0GabPTbWK4o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Tokenization\n",
        "Now, we need to split the full rulings into individual phrases. To do this, we train a custom tokenizer."
      ],
      "metadata": {
        "id": "1tAv1RecWmfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_text = \"\"\n",
        "\n",
        "for text in data[\"Clean_Text\"]:\n",
        "    long_text += text\n",
        "\n",
        "trainer = PunktTrainer()\n",
        "trainer.INCLUDE_ALL_COLLOCS = True\n",
        "trainer.train(long_text)\n",
        "\n",
        "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
      ],
      "metadata": {
        "id": "zlfK8qW7dbg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can apply it to the texts."
      ],
      "metadata": {
        "id": "FpARovr0dgIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_ids = []\n",
        "all_sentences = []\n",
        "\n",
        "for idx, text in enumerate(data[\"Clean_Text\"]):\n",
        "    sentences = tokenizer.tokenize(text)\n",
        "    all_sentences.extend(sentences)\n",
        "    doc_ids.extend([idx] * len(sentences))"
      ],
      "metadata": {
        "id": "picaeVxoWs_l"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "1u9LO_IHdynL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERTopic Modeling"
      ],
      "metadata": {
        "id": "Ch5BD3HJdzRY"
      }
    }
  ]
}